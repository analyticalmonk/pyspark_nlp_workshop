{"cells":[{"cell_type":"markdown","source":["In this module, we'll explore Named Entity Recognition (NER) and Sentiment Analysis using Spark NLP with the Amazon reviews dataset. We'll identify the most common named entities mentioned in the reviews and analyze the overall sentiment of those reviews."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5a2d4b82-9d14-4969-a4e4-4da673ee4dd8","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Import the necessary modules from PySpark and Spark NLP. These include functions and types from PySpark, and various annotators and base classes from Spark NLP."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c8b17c74-8c7b-48a1-92fc-4abc97b349b3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import StringType\n\nimport sparknlp\nfrom sparknlp.annotator import *\nfrom sparknlp.base import *"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6bf3277c-5208-4d71-aeaa-104ff9468c0f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["text_data_path = \"dbfs:/databricks-datasets/amazon/data20K\"\ntext_df = spark.read.parquet(text_data_path, header=True, inferSchema=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"09cba179-d734-44a7-9e57-20ea843d10d4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Named Entity Recognition\n\nOnce our data is loaded, we're ready to start processing it. For our first task, we're going to identify named entities in the reviews. Named entities are real-world objects such as persons, locations, organizations, and so on, that can be denoted with a proper name.\n\nNamed Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities in text into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n\nWe'll create a processing pipeline with Spark NLP to do this. A pipeline is a sequence of stages where each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"29996eed-8e69-44de-b9db-b4fb50d12e3c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# NER pipeline to identify and classify named entities in the reviews:\n\ndocument_assembler = DocumentAssembler() \\\n    .setInputCol(\"review\") \\\n    .setOutputCol(\"document\")\n\nsentence_detector = SentenceDetector() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"sentences\")\n\ntokenizer = Tokenizer() \\\n    .setInputCols([\"sentences\"]) \\\n    .setOutputCol(\"tokens\")\n\nembeddings = WordEmbeddingsModel.pretrained(\"glove_100d\") \\\n    .setInputCols([\"document\", \"tokens\"]) \\\n    .setOutputCol(\"embeddings\")\n\nner_model = NerDLModel.pretrained(\"ner_dl\", \"en\") \\\n    .setInputCols([\"sentences\", \"tokens\", \"embeddings\"]) \\\n    .setOutputCol(\"ner_tags\")\n\nner_converter = NerConverter() \\\n    .setInputCols([\"sentences\", \"tokens\", \"ner_tags\"]) \\\n    .setOutputCol(\"entities\")\n\npipeline = Pipeline(stages=[\n    document_assembler,\n    sentence_detector,\n    tokenizer,\n    embeddings,\n    ner_model,\n    ner_converter\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d13c9bf0-d8bf-4ef2-83b4-ac6ee2eee2b1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We're using the GloVe embeddings pretrained on 6 billion words (\"glove_100d\") to generate **word embeddings** which are then provided as input during named entity recognition. You can replace \"glove_100d\" with [any other embeddings](https://nlp.johnsnowlabs.com/models?type=model&q=glove) you prefer.\n\nThe **NerDLModel** is a Named Entity Recognition model trained by a deep learning approach. It assigns to every word in a text a tag that signifies whether the word is a named entity or not, and what category it belongs to. It's trained using a variety of neural network architectures, such as Char CNNs - BiLSTM - CRF and Bert - BiLSTM - CRF.  \nIn the code above, we are loading a pretrained NER model (named \"ner_dl\") for English (\"en\"). We set the input columns to be \"sentences\" and \"tokens\". This means that the model will perform NER on the tokenized sentences from our text. The output of this stage is a new column, \"ner_tags\", which contains the NER tags assigned by the model to each word.\n\nThe **NerConverter** is used to convert the output from the NerDLModel into a more readable format. It groups together consecutive tokens with the same NER tag into single entities and classifies them according to their tag.  \nWe're setting the input columns to be \"sentences\", \"tokens\", and \"ner_tags\". This means that the NerConverter will take the tokenized sentences and their corresponding NER tags as input. The output of this stage is a new column, \"entities\", which contains the named entities extracted from the text. Each entity is represented as a chunk of text along with its NER tag."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"149e4510-209d-4bdb-9f42-0359c8e81dce","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["We will now transform the reviews DataFrame using the NER pipeline:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cbe591a9-ad13-42e2-83ed-302539d39b22","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["pipeline_model = pipeline.fit(text_df)\nner_result_df = pipeline_model.transform(text_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"68edda99-4a9d-4186-8dbb-fe490cdacd1b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ner_result_df.show(4)\n\n# ner_result_df.select(\"review\", \"entities\").show(2, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"88cd72ba-9bef-4db1-bf76-6545cd29d7cf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Spark NLP also offers pretrained pipeline. They are a pre-built set of transformations that have been trained on a large dataset and saved, so they can be reused. This is a great way to get started quickly with NLP tasks like Named Entity Recognition (NER), sentiment analysis, and more.\n\nTo use a pretrained pipeline, you don't need to assemble each individual component. Instead, you can load the entire pipeline at once. One catch is that pretrained pipelines assume that the input column is named \"text\".\n\nHere's an example of how you can do this:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4715d87-8407-4c63-abc6-5c892fd2b9c5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# from sparknlp.pretrained import PretrainedPipeline\n\n# Load a pretrained pipeline\n# pipeline = PretrainedPipeline(\"recognize_entities_dl\", lang=\"en\")\n\n# Rename the target column - \"review\" -> \"text\"\n# text_df_2 = test_df.withColumnRenamed(\"review\", \"text\")\n\n# Use the pipeline to annotate a DataFrame\n# result_df = pipeline.transform(text_df_2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b9e5e56d-2d12-4313-b16c-b69053d49fea","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["- We import the PretrainedPipeline class from the sparknlp.pretrained module.\n- We create an instance of the PretrainedPipeline class by calling the constructor and passing the name of the pretrained pipeline we want to use (\"recognize_entities_dl\") and the language (\"en\").\n- We rename the input column from \"review\" to \"text\".\n- We use the annotate method of the PretrainedPipeline instance to transform our DataFrame. The annotate method takes two arguments: the DataFrame to transform, and the name of the column in the DataFrame that contains the text to process."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"36bb1be1-3124-4a19-a633-2949f3516d8e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Our next step is to extract the most common named entities from our processed data. \n\nWe'll use groupBy and count to find the most common ones."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3cdd1574-9259-469e-97ea-08360bdd076a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["named_entities = ner_result_df.select(\"entities.result\").withColumnRenamed(\"result\", \"named_entities\")\n\nnamed_entities.show(5, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c7d122c4-c8d1-4d15-90a5-a4a9e2d4ab62","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import count, desc\n\ntop_entities = named_entities.groupBy(\"named_entities\").agg(count(\"*\").alias(\"count\")).sort(desc(\"count\")).limit(10)\ntop_entities.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"28fc4ca4-3a57-4a55-99b6-38b041eca19c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["- We rename the column containing the list of NER results to \"named_entities\". \n- Then we count the number of occurrences for each entity and sort them in descending order.\n- Finally, we limit our output to the top 10 entities."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dea70124-ed4d-47b8-89b4-700c07791a08","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Sentiment analysis\n\n\nNext, we build another pipeline, this time for sentiment analysis. This pipeline is similar to our NER pipeline, but instead of the NER model, we use a pretrained sentiment analysis model."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8405d927-3675-41e0-9589-75805d14e03f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["document_assembler = DocumentAssembler() \\\n    .setInputCol(\"review\") \\\n    .setOutputCol(\"document\")\n\nsentence_detector = SentenceDetector() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"sentences\")\n\ntokenizer = Tokenizer() \\\n    .setInputCols([\"sentences\"]) \\\n    .setOutputCol(\"tokens\")\n\nsentiment_model = SentimentDLModel.pretrained(\"sentimentdl_use_imdb\", \"en\") \\\n    .setInputCols([\"sentences\", \"tokens\"]) \\\n    .setOutputCol(\"sentiment\")\n\npipeline = Pipeline(stages=[\n    document_assembler,\n    sentence_detector,\n    tokenizer,\n    sentiment_model\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3aa705f7-543b-42d9-b018-ce1f0151a742","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can now fit and transform our filtered reviews DataFrame using this new pipeline."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0c798b10-dd19-46fe-8b7a-ad5e8001901c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["pipeline_model = pipeline.fit(text_df)\nsentiment_result_df = pipeline_model.transform(text_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"19d52005-b1d4-4dbb-b5fe-b204419289ac","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Module 3: Advanced NLP with Spark NLP","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
